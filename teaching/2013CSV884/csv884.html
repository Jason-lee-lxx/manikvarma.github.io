<html>

<head>
  <title>CSV 884</title>
  <meta name="keyword" content="manik, varma, teaching IIT CSV 884"> 
</head>

<body>
<center><h1><b>CSV 884 - Supervised Learning with Kernels</b></h1></center>

<hr size=4>
<table>
<tr>
  <td>Instructor</td>
  <td><a href="../../index.html">Manik Varma</a></td>
</tr>
<tr>
  <td>Co-ordinator</td>
  <td><a href="http://www.cse.iitd.ac.in/~kkb/">K. K. Biswas</a></td>
</tr>
<tr>
  <td>Student Volunteer</td>
  <td><a href="mailto:rahulkumar.iitd57@gmail.com">Rahul Kumar</a></td>
</tr>
<tr>
  <td>Credits</td>
  <td>1</td>
</tr>
<tr>
  <td>Classroom</td>
  <td>201, IIA, Bharti Building </td>
</tr>
<tr>
  <td>Timings</td>
  <td>2:00 - 3:30 PM on Tuesdays and Fridays.</td>
</tr>
<tr>
  <td>Kaggle Competitions &nbsp; &nbsp; &nbsp;</td>
  <td>
    <a href="https://inclass.kaggle.com/c/iit-delhi-csv884-12">Data Set 1</a> &nbsp;
    <a href="https://inclass.kaggle.com/c/iit-delhi-csv884-ml-2">Data Set 2</a> &nbsp;
    <a href="https://inclass.kaggle.com/c/iit-delhi-csv884-ml-3">Data Set 3</a> &nbsp;
    <a href="https://inclass.kaggle.com/c/iit-delhi-csv884-ml-4">Data Set 4</a> &nbsp;
    <a href="https://inclass.kaggle.com/c/iit-delhi-csv884-ml-5">Data Set 5</a> &nbsp;
    <a href="https://inclass.kaggle.com/c/iit-delhi-csv884-ml-6">Data Set 6</a>
</td>

</tr>

</table>
<hr size=4><br>


<p> 
Support Vector Machines (SVMs) are one of the most popular tools used to tackle supervised machine learning problems. They have defined the state-of-the-art on multiple benchmark tasks and are easy to use requiring relatively little machine learning expertise. In this course, we will take a detailed look at the area of supervised machine learning from an SVM perspective. We will start by introducing machine learning and SVM basics and will then go into four current areas of SVM research: (a) formulations; (b) kernels and kernel learning; (c) optimization and (d) prediction.  Emphasis will be placed on both the mathematical treatment of SVMs as well as the practical aspects of optimization and implementation. Students are expected to be comfortable with linear algebra, probability/statistics and coding in C and Matlab. By the end of the course, students should be able to use SVMs to solve real world machine learning problems as well as take up research projects extending SVMs in novel directions.
</p>

<p>
<font size=5 color="red"><b>Lectures</b></font>
</p>

<table width="100%">
<tr>
  <td>Lecture 1 (30-07-2013)<br>
      Introduction to ML, features, over fitting and generalization, noise and prior knowledge &nbsp;
  </td>
  <td>
    <a href="Notes/lec1.pdf">Lecture 1 notes</a><br>
    Chapter 1 of&nbsp;&nbsp;DHS
  </td>
</tr>

<tr>
  <td>Lecture 2 (02-08-2013)<br>
      Bayesian vs MAP vs ML approaches &nbsp;
  </td>
  <td>
    <a href="Notes/lec2.pdf">Lecture 2 notes</a><br>
    Chapter 1 of&nbsp;&nbsp;PRML<br>
    Chapter 3 of&nbsp;&nbsp;DHS
  </td>
</tr>

<tr>
  <td>Lectures 3 (06-08-2013) and 4 (13-08-2013)<br>
      Generative vs Discriminative approaches &nbsp;
  </td>
  <td>
    <a href="Notes/lec3.pdf">Lecture 3 notes</a><br>
    <a href="Notes/lec4.pdf">Lecture 4 notes</a><br>
    <a href="http://www.cs.cmu.edu/~tom/mlbook/NBayesLogReg.pdf">Chapter</a> from Mitchell<br>
    <a href="http://ai.stanford.edu/~ang/papers/nips01-discriminativegenerative.pdf">Ng & Jordan NIPS 01</a>
  </td>
</tr>

<tr>
  <td>Lectures 5 (16-08-2013) and 6 (23-08-2013)<br>
      Linear SVMs<br>
      Primal optimization - gradient descent and stochastic gradient descent&nbsp;
  </td>
  <td>
    <a href="Notes/lec5.pdf">Lecture 5 notes</a><br>
    <a href="Notes/lec6.pdf">Lecture 6 notes</a><br>
    <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.18.1083">Tutorial by Chris Burges</a><br>
    <a href="http://eprints.pascal-network.org/archive/00004062/01/ShalevSiSr07.pdf">Pegasos</a>
  </td>
</tr>

<tr>
  <td> <br> </td>
</tr>

<tr>
  <td>Lecture 7 (15-10-2013)<br>
      Linear SVMs continued<br>
      Dual optimization - dual co-ordinate ascent&nbsp;
  </td>
  <td>
    <a href="Notes/lec7.pdf">Lecture 7 notes</a><br>
    <a href="http://ntu.csie.org/~cjlin/papers/cddual.pdf
">Liblinear</a>
  </td>
</tr>

<tr>
  <td> <br> </td>
</tr>

<tr>
  <td>Lecture 8 (18-10-2013)<br>
      Linear SVMs continued<br>
      Cutting plane optimization&nbsp;
  </td>
  <td>
    <a href="Notes/lec8.pdf">Lecture 8 notes</a><br>
    <a href="http://www.cs.cornell.edu/people/tj/publications/joachims_06a.pdf">Joachims KDD 06</a>
  </td>
</tr>

<tr>
  <td> <br> </td>
</tr>

<tr>
  <td>Lectures 9 (29-10-2013) and 10 (01-01-2013)<br>
      Kernels<br>
      Block co-ordinate ascent optimization&nbsp;
  </td>
  <td>
    <a href="Notes/lec9.pdf">Lecture 9 notes</a><br>
    <a href="Notes/lec10.pdf">Lecture 10 notes</a><br>
    <a href="http://www.csie.ntu.edu.tw/~cjlin/papers/quadworkset.pdf
">SMO - Fan, Chen and Lin JMLR 05</a>
  </td>
</tr>

</table>

<p>
<font size=5 color="red"><b>Code</b></font>
</p>

<p>
Most code was written five minutes before the start of each lectureand comes with no guarantees, comments or documentation. In particular, no attempt has been made to bulletproof the code. For instance, if there is no feasible solution for your parameter settings then the figure plotting subroutines will crash (the LR and SVM learning routines should be stable). In any case, run the code at your own peril. 
</p>

<ul type="circle">
  <li><a href="Code/COMTools.tgz">Some common MATLAB tools needed for
  the demos</a> (you'll need the Optimization Toolbox ver 4.2 or
  higher for fmincon.LBFGS for Logistic Regression)</li> 
  <li><a href="Code/LRNdemos.tgz">MATLAB code for demos</a>
    <ul type="circle">
      <li>Na&iuml;ve Bayes</li>
      <li>Regularized Logistic Regression (code is kernelized and
      displays dual weights)</li>  
      <li>Na&iuml;ve Bayes vs Logistic Regression</li>
      <li>Multi-class Logistic Regression (Multinomial, 1-vs-All,
      1-vs-1 DAG, 1-vs-1 majority vote)</li>
      <li>Linear SVMs vs Logistic Regression</li>
      <li>Non-linear SVMs</li>
      <li>Multi-class SVMs (multi-class hinge loss, Multinomial Logistic Regression, 1-vs-All SVM, 1-vs-1 DAG SVM, 1-vs-1 majority vote SVM)</li>
    </ul>
  </li> 
</ul>

<p>
<font size=5 color="red"><b>Recommended Reading</b></font>
</p>

<ul type="circle">
  <li> 
    D.&nbsp;Bertsekas.
    <a href="http://www.athenasc.com/nonlinbook.html"><em>Nonlinear Programming</em></a>.
    Athena Scientific, 1999.
  </li>

  <li> 
    C.&nbsp;M. Bishop.
    <a href="http://research.microsoft.com/~cmbishop/PRML/index.htm"><em>Pattern Recognition and Machine Learning</em></a>.
    Springer, 2006.
  </li>

  <li> 
    S.&nbsp;Boyd and L.&nbsp;Vandenberghe.
    <a href="http://www.stanford.edu/~boyd/cvxbook/"><em>Convex Optimization</em></a>.
    Cambridge University Press, 2004.
  </li>

  <li> 
    N.&nbsp;Cristianini and J.&nbsp;Shawe-Taylor.
    <a href="http://www.support-vector.net/"><em>An Introduction to Support Vector Machines and
    Other Kernel-based Learning Methods</em></a>. 
    Cambridge University Press, 2000.
  </li>

  <li> 
    R.&nbsp;O. Duda, P.&nbsp;E. Hart, and D.&nbsp;G. Stork.
    <a href="http://as.wiley.com/WileyCDA/WileyTitle/productCd-0471056693.html"><em>Pattern Classification</em></a>.
    John Wiley and Sons, second edition, 2001.
  </li>

  <li> 
    T.&nbsp;Hastie, R.&nbsp;Tibshirani, and J.&nbsp;Friedman.
    <a
    href="http://www-stat.stanford.edu/~tibs/ElemStatLearn/"><em>The
    Elements of Statistical Learning</em></a>.
    Springer, second edition, 2009.
  </li>

  <li> 
    T.&nbsp;Mitchell.
    <a href="http://www.cs.cmu.edu/afs/cs.cmu.edu/user/mitchell/ftp/mlbook.html"><em>Machine Learning</em></a>.
    McGraw Hill, 1997.
  </li>

  <li> 
    B.&nbsp;Scholkopf and A.&nbsp;Smola.
    <a href="http://www.learning-with-kernels.org/"><em>Learning with Kernels</em></a>.
    MIT Press, 2002.</dd>
  </li>
</ul>

<hr>
<a href="../../index.html">Back to Manik's Home Page</a>

</body>

</html>


